{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "4Lti6wr6W1Gp"
   },
   "source": [
    "Step 1. Data Collection and Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x9FhOakxWvGr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "g4gx9p1WXB8z",
    "outputId": "e62d7c42-9ca3-4a9f-d3a6-ef5c31e9b659"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                                Text           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try reading the CSV file with a different encoding\n",
    "df = pd.read_csv('dataset_of text classification.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "b6Xw4OhdZgAk",
    "outputId": "ea3924f6-db0c-40a1-9307-e6acec647ab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4-v54KipaMzO"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z_vShJd6afyx"
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: x.lower()) #for converting it to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvlDpX8zbIzU",
    "outputId": "e1b24216-365c-4c97-ba29-ba438e287079"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ralphonseraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ralphonseraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words and Special Characters\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Text'] = df['Text'].apply(lambda x: ' '.join ([word for word in word_tokenize(x) if word.isalnum() and word not in stop_words]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtJ0EiczdTHh"
   },
   "source": [
    "Step 2 - Model Architecture and Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oIbceViEbTcO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "class CustomGPT2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomGPT2, self).__init__()\n",
    "        config = GPT2Config.from_pretrained('gpt2')\n",
    "        self.gpt2 = GPT2Model(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "# Define the CustomGPT2 model\n",
    "# class CustomGPT2(nn.Module):\n",
    "#     def __init__(self, num_labels):\n",
    "#         super(CustomGPT2, self).__init__()\n",
    "#         self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.classifier = nn.Linear(self.gpt2.config.hidden_size, num_labels)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask=None):\n",
    "#         outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         sequence_output = outputs.last_hidden_state #whatever outputs your gpt2 is giving that you are picking the last hidden state.\n",
    "#         pooled_output = sequence_output[:, -1, :]\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbbmZA51fWbQ"
   },
   "source": [
    "Step 3 - Training Script Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHhDTDuMfVba",
    "outputId": "33bc1209-5c2a-40a2-b416-6d8a9dc73a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral' 'Positive' 'Extremely Negative' 'Negative' 'Extremely Positive']\n"
     ]
    }
   ],
   "source": [
    "# # Define a mapping from sentiment labels to numerical labels\n",
    "# sentiment_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "\n",
    "# Check unique values in the 'Sentiment' column\n",
    "unique_sentiments = df['Sentiment'].unique()\n",
    "print(unique_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BUL8H4tOFWjO"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2Config, GPT2Model, GPT2Tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# Define a mapping from sentiment labels to numerical labels\n",
    "sentiment_mapping = {\n",
    "    'Extremely Negative': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3,\n",
    "    'Extremely Positive': 4\n",
    "}\n",
    "\n",
    "# Tokenize the text using the GPT-2 tokenizer and cast to torch.LongTensor\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenized_text = df['Text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "max_length = 64\n",
    "padded_text = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in tokenized_text], batch_first=True, padding_value=0)[:, :max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sgFycKO6GEmp"
   },
   "outputs": [],
   "source": [
    "# # Assuming sentiment_mapping is a dictionary mapping sentiment labels to numerical labels\n",
    "# sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "\n",
    "\n",
    "# Convert sentiment labels to numerical labels and cast to torch.LongTensor\n",
    "labels_tensor = torch.tensor(df['Sentiment'].map(sentiment_mapping).values, dtype=torch.long)\n",
    "\n",
    "# Create a PyTorch dataset and DataLoader\n",
    "dataset = TensorDataset(padded_text, labels_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define your CustomGPT2 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CustomGPT2().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hZKnRNBgGJRj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 0, Batch 0, Loss: 7.358343601226807\n",
      "INFO:__main__:Epoch 0, Batch 10, Loss: 1.8029873371124268\n",
      "INFO:__main__:Epoch 0, Batch 20, Loss: 1.8454762697219849\n",
      "INFO:__main__:Epoch 0, Batch 30, Loss: 1.7500286102294922\n",
      "INFO:__main__:Epoch 0, Batch 40, Loss: 1.844016194343567\n",
      "INFO:__main__:Epoch 0, Batch 50, Loss: 1.9717369079589844\n",
      "INFO:__main__:Epoch 0, Batch 60, Loss: 1.6280494928359985\n",
      "INFO:__main__:Epoch 0, Batch 70, Loss: 1.7735522985458374\n",
      "INFO:__main__:Epoch 0, Batch 80, Loss: 1.9819430112838745\n",
      "INFO:__main__:Epoch 0, Batch 90, Loss: 1.6509259939193726\n",
      "INFO:__main__:Epoch 0, Batch 100, Loss: 1.6819223165512085\n",
      "INFO:__main__:Epoch 0, Batch 110, Loss: 1.579703688621521\n",
      "INFO:__main__:Epoch 0, Batch 120, Loss: 1.7213611602783203\n",
      "INFO:__main__:Epoch 0, Batch 130, Loss: 1.682446002960205\n",
      "INFO:__main__:Epoch 0, Batch 140, Loss: 1.6632447242736816\n",
      "INFO:__main__:Epoch 0, Batch 150, Loss: 1.5399305820465088\n",
      "INFO:__main__:Epoch 0, Batch 160, Loss: 1.6873606443405151\n",
      "INFO:__main__:Epoch 0, Batch 170, Loss: 1.7988719940185547\n",
      "INFO:__main__:Epoch 0, Batch 180, Loss: 1.6301759481430054\n",
      "INFO:__main__:Epoch 0, Batch 190, Loss: 1.825890064239502\n",
      "INFO:__main__:Epoch 0, Batch 200, Loss: 1.58969247341156\n",
      "INFO:__main__:Epoch 0, Batch 210, Loss: 1.6237469911575317\n",
      "INFO:__main__:Epoch 0, Batch 220, Loss: 1.7052768468856812\n",
      "INFO:__main__:Epoch 0, Batch 230, Loss: 1.628689169883728\n",
      "INFO:__main__:Epoch 0, Batch 240, Loss: 1.5179815292358398\n",
      "INFO:__main__:Epoch 0, Batch 250, Loss: 1.5546705722808838\n",
      "INFO:__main__:Epoch 0, Batch 260, Loss: 1.5890429019927979\n",
      "INFO:__main__:Epoch 0, Batch 270, Loss: 1.8110682964324951\n",
      "INFO:__main__:Epoch 0, Batch 280, Loss: 1.6113929748535156\n",
      "INFO:__main__:Epoch 0, Batch 290, Loss: 1.5372265577316284\n",
      "INFO:__main__:Epoch 0, Batch 300, Loss: 1.5923211574554443\n",
      "INFO:__main__:Epoch 0, Batch 310, Loss: 1.5581010580062866\n",
      "INFO:__main__:Epoch 0, Batch 320, Loss: 1.5113831758499146\n",
      "INFO:__main__:Epoch 0, Batch 330, Loss: 1.5414220094680786\n",
      "INFO:__main__:Epoch 0, Batch 340, Loss: 1.466287612915039\n",
      "INFO:__main__:Epoch 0, Batch 350, Loss: 1.4545246362686157\n",
      "INFO:__main__:Epoch 0, Batch 360, Loss: 1.723924160003662\n",
      "INFO:__main__:Epoch 0, Batch 370, Loss: 1.5524765253067017\n",
      "INFO:__main__:Epoch 0, Batch 380, Loss: 1.5161079168319702\n",
      "INFO:__main__:Epoch 0, Batch 390, Loss: 1.4673435688018799\n",
      "INFO:__main__:Epoch 0, Batch 400, Loss: 1.4691948890686035\n",
      "INFO:__main__:Epoch 0, Batch 410, Loss: 1.4801945686340332\n",
      "INFO:__main__:Epoch 0, Batch 420, Loss: 1.5513041019439697\n",
      "INFO:__main__:Epoch 0, Batch 430, Loss: 1.6970311403274536\n",
      "INFO:__main__:Epoch 0, Batch 440, Loss: 1.52775239944458\n",
      "INFO:__main__:Epoch 0, Batch 450, Loss: 1.606269359588623\n",
      "INFO:__main__:Epoch 0, Batch 460, Loss: 1.751478910446167\n",
      "INFO:__main__:Epoch 0, Batch 470, Loss: 1.7135051488876343\n",
      "INFO:__main__:Epoch 0, Batch 480, Loss: 1.649373173713684\n",
      "INFO:__main__:Epoch 0, Batch 490, Loss: 1.5576226711273193\n",
      "INFO:__main__:Epoch 0, Batch 500, Loss: 1.4169819355010986\n",
      "INFO:__main__:Epoch 0, Batch 510, Loss: 1.5689502954483032\n",
      "INFO:__main__:Epoch 0, Batch 520, Loss: 1.4891048669815063\n",
      "INFO:__main__:Epoch 0, Batch 530, Loss: 1.536547064781189\n",
      "INFO:__main__:Epoch 0, Batch 540, Loss: 1.4190322160720825\n",
      "INFO:__main__:Epoch 0, Batch 550, Loss: 1.4970341920852661\n",
      "INFO:__main__:Epoch 0, Batch 560, Loss: 1.3770610094070435\n",
      "INFO:__main__:Epoch 0, Batch 570, Loss: 1.295351266860962\n",
      "INFO:__main__:Epoch 0, Batch 580, Loss: 1.6577502489089966\n",
      "INFO:__main__:Epoch 0, Batch 590, Loss: 1.5590691566467285\n",
      "INFO:__main__:Epoch 0, Batch 600, Loss: 1.5388669967651367\n",
      "INFO:__main__:Epoch 0, Batch 610, Loss: 1.4557244777679443\n",
      "INFO:__main__:Epoch 0, Batch 620, Loss: 1.6996803283691406\n",
      "INFO:__main__:Epoch 0, Batch 630, Loss: 1.696478009223938\n",
      "INFO:__main__:Epoch 0, Batch 640, Loss: 1.5457139015197754\n",
      "INFO:__main__:Epoch 0, Batch 650, Loss: 1.7336101531982422\n",
      "INFO:__main__:Epoch 0, Batch 660, Loss: 1.3988966941833496\n",
      "INFO:__main__:Epoch 0, Batch 670, Loss: 1.392205834388733\n",
      "INFO:__main__:Epoch 0, Batch 680, Loss: 1.6075135469436646\n",
      "INFO:__main__:Epoch 0, Batch 690, Loss: 1.5082628726959229\n",
      "INFO:__main__:Epoch 0, Batch 700, Loss: 1.6070964336395264\n",
      "INFO:__main__:Epoch 0, Batch 710, Loss: 1.4510103464126587\n",
      "INFO:__main__:Epoch 0, Batch 720, Loss: 1.4270613193511963\n",
      "INFO:__main__:Epoch 0, Batch 730, Loss: 1.5259780883789062\n",
      "INFO:__main__:Epoch 0, Batch 740, Loss: 1.5036990642547607\n",
      "INFO:__main__:Epoch 0, Batch 750, Loss: 1.5550899505615234\n",
      "INFO:__main__:Epoch 0, Batch 760, Loss: 1.574663519859314\n",
      "INFO:__main__:Epoch 0, Batch 770, Loss: 1.7255468368530273\n",
      "INFO:__main__:Epoch 0, Batch 780, Loss: 1.6866010427474976\n",
      "INFO:__main__:Epoch 0, Batch 790, Loss: 1.5192183256149292\n",
      "INFO:__main__:Epoch 0, Batch 800, Loss: 1.4523396492004395\n",
      "INFO:__main__:Epoch 0, Batch 810, Loss: 1.5602507591247559\n",
      "INFO:__main__:Epoch 0, Batch 820, Loss: 1.3674753904342651\n",
      "INFO:__main__:Epoch 0, Batch 830, Loss: 1.4008747339248657\n",
      "INFO:__main__:Epoch 0, Batch 840, Loss: 1.4826589822769165\n",
      "INFO:__main__:Epoch 0, Batch 850, Loss: 1.4425855875015259\n",
      "INFO:__main__:Epoch 0, Batch 860, Loss: 1.6193236112594604\n",
      "INFO:__main__:Epoch 0, Batch 870, Loss: 1.617820382118225\n",
      "INFO:__main__:Epoch 0, Batch 880, Loss: 1.247902274131775\n",
      "INFO:__main__:Epoch 0, Batch 890, Loss: 1.3089022636413574\n",
      "INFO:__main__:Epoch 0, Batch 900, Loss: 1.4615148305892944\n",
      "INFO:__main__:Epoch 0, Batch 910, Loss: 1.3629010915756226\n",
      "INFO:__main__:Epoch 0, Batch 920, Loss: 1.5262601375579834\n",
      "INFO:__main__:Epoch 0, Batch 930, Loss: 1.5449786186218262\n",
      "INFO:__main__:Epoch 0, Batch 940, Loss: 1.5487444400787354\n",
      "INFO:__main__:Epoch 0, Batch 950, Loss: 1.4330018758773804\n",
      "INFO:__main__:Epoch 0, Batch 960, Loss: 1.6919913291931152\n",
      "INFO:__main__:Epoch 0, Batch 970, Loss: 1.6674480438232422\n",
      "INFO:__main__:Epoch 0, Batch 980, Loss: 1.4869482517242432\n",
      "INFO:__main__:Epoch 0, Batch 990, Loss: 1.528976559638977\n",
      "INFO:__main__:Epoch 0, Batch 1000, Loss: 1.213017463684082\n",
      "INFO:__main__:Epoch 0, Batch 1010, Loss: 1.331674337387085\n",
      "INFO:__main__:Epoch 0, Batch 1020, Loss: 1.3528833389282227\n",
      "INFO:__main__:Epoch 0, Batch 1030, Loss: 1.3712717294692993\n",
      "INFO:__main__:Epoch 0, Batch 1040, Loss: 1.4520748853683472\n",
      "INFO:__main__:Epoch 0, Batch 1050, Loss: 1.692280888557434\n",
      "INFO:__main__:Epoch 0, Batch 1060, Loss: 1.6264457702636719\n",
      "INFO:__main__:Epoch 0, Batch 1070, Loss: 1.3167310953140259\n",
      "INFO:__main__:Epoch 0, Batch 1080, Loss: 1.3620284795761108\n",
      "INFO:__main__:Epoch 0, Batch 1090, Loss: 1.2985271215438843\n",
      "INFO:__main__:Epoch 0, Batch 1100, Loss: 1.4788331985473633\n",
      "INFO:__main__:Epoch 0, Batch 1110, Loss: 1.4188026189804077\n",
      "INFO:__main__:Epoch 0, Batch 1120, Loss: 1.650170922279358\n",
      "INFO:__main__:Epoch 0, Batch 1130, Loss: 1.3709931373596191\n",
      "INFO:__main__:Epoch 0, Batch 1140, Loss: 1.2829409837722778\n",
      "INFO:__main__:Epoch 0, Batch 1150, Loss: 1.3691293001174927\n",
      "INFO:__main__:Epoch 0, Batch 1160, Loss: 1.300627589225769\n",
      "INFO:__main__:Epoch 0, Batch 1170, Loss: 1.373323917388916\n",
      "INFO:__main__:Epoch 0, Batch 1180, Loss: 1.420754075050354\n",
      "INFO:__main__:Epoch 0, Batch 1190, Loss: 1.2572625875473022\n",
      "INFO:__main__:Epoch 0, Batch 1200, Loss: 1.4973993301391602\n",
      "INFO:__main__:Epoch 0, Batch 1210, Loss: 1.5446799993515015\n",
      "INFO:__main__:Epoch 0, Batch 1220, Loss: 1.4478917121887207\n",
      "INFO:__main__:Epoch 0, Batch 1230, Loss: 1.5228784084320068\n",
      "INFO:__main__:Epoch 0, Batch 1240, Loss: 1.5989488363265991\n",
      "INFO:__main__:Epoch 0, Batch 1250, Loss: 1.221278429031372\n",
      "INFO:__main__:Epoch 0, Batch 1260, Loss: 1.485797643661499\n",
      "INFO:__main__:Epoch 0, Batch 1270, Loss: 1.6671713590621948\n",
      "INFO:__main__:Epoch 0, Batch 1280, Loss: 1.307664155960083\n",
      "INFO:__main__:Epoch 0, Loss: 1.5494050272880087, Accuracy: 0.3145515951114027\n",
      "INFO:__main__:Saved checkpoint to 'model_checkpoint_epoch_0.pth'.\n",
      "INFO:__main__:Epoch 1, Batch 0, Loss: 1.4264261722564697\n",
      "INFO:__main__:Epoch 1, Batch 10, Loss: 1.2774773836135864\n",
      "INFO:__main__:Epoch 1, Batch 20, Loss: 1.5662859678268433\n",
      "INFO:__main__:Epoch 1, Batch 30, Loss: 1.4412516355514526\n",
      "INFO:__main__:Epoch 1, Batch 40, Loss: 1.415199637413025\n",
      "INFO:__main__:Epoch 1, Batch 50, Loss: 1.3178648948669434\n",
      "INFO:__main__:Epoch 1, Batch 60, Loss: 1.5924042463302612\n",
      "INFO:__main__:Epoch 1, Batch 70, Loss: 1.4282231330871582\n",
      "INFO:__main__:Epoch 1, Batch 80, Loss: 1.426201343536377\n",
      "INFO:__main__:Epoch 1, Batch 90, Loss: 1.3068575859069824\n",
      "INFO:__main__:Epoch 1, Batch 100, Loss: 1.4030405282974243\n",
      "INFO:__main__:Epoch 1, Batch 110, Loss: 1.2041285037994385\n",
      "INFO:__main__:Epoch 1, Batch 120, Loss: 1.4056675434112549\n",
      "INFO:__main__:Epoch 1, Batch 130, Loss: 1.2398638725280762\n",
      "INFO:__main__:Epoch 1, Batch 140, Loss: 1.6562303304672241\n",
      "INFO:__main__:Epoch 1, Batch 150, Loss: 1.3884871006011963\n",
      "INFO:__main__:Epoch 1, Batch 160, Loss: 1.5263431072235107\n",
      "INFO:__main__:Epoch 1, Batch 170, Loss: 1.39641273021698\n",
      "INFO:__main__:Epoch 1, Batch 180, Loss: 1.574087142944336\n",
      "INFO:__main__:Epoch 1, Batch 190, Loss: 1.6441500186920166\n",
      "INFO:__main__:Epoch 1, Batch 200, Loss: 1.4041835069656372\n",
      "INFO:__main__:Epoch 1, Batch 210, Loss: 1.4491468667984009\n",
      "INFO:__main__:Epoch 1, Batch 220, Loss: 1.4539284706115723\n",
      "INFO:__main__:Epoch 1, Batch 230, Loss: 1.4133052825927734\n",
      "INFO:__main__:Epoch 1, Batch 240, Loss: 1.61215341091156\n",
      "INFO:__main__:Epoch 1, Batch 250, Loss: 1.3189582824707031\n",
      "INFO:__main__:Epoch 1, Batch 260, Loss: 1.5693055391311646\n",
      "INFO:__main__:Epoch 1, Batch 270, Loss: 1.5827231407165527\n",
      "INFO:__main__:Epoch 1, Batch 280, Loss: 1.3965109586715698\n",
      "INFO:__main__:Epoch 1, Batch 290, Loss: 1.3122258186340332\n",
      "INFO:__main__:Epoch 1, Batch 300, Loss: 1.6372478008270264\n",
      "INFO:__main__:Epoch 1, Batch 310, Loss: 1.2555921077728271\n",
      "INFO:__main__:Epoch 1, Batch 320, Loss: 1.458810567855835\n",
      "INFO:__main__:Epoch 1, Batch 330, Loss: 1.4986543655395508\n",
      "INFO:__main__:Epoch 1, Batch 340, Loss: 1.4332914352416992\n",
      "INFO:__main__:Epoch 1, Batch 350, Loss: 1.3426824808120728\n",
      "INFO:__main__:Epoch 1, Batch 360, Loss: 1.3950557708740234\n",
      "INFO:__main__:Epoch 1, Batch 370, Loss: 1.5303423404693604\n",
      "INFO:__main__:Epoch 1, Batch 380, Loss: 1.4102171659469604\n",
      "INFO:__main__:Epoch 1, Batch 390, Loss: 1.419714331626892\n",
      "INFO:__main__:Epoch 1, Batch 400, Loss: 1.5965677499771118\n",
      "INFO:__main__:Epoch 1, Batch 410, Loss: 1.3863404989242554\n",
      "INFO:__main__:Epoch 1, Batch 420, Loss: 1.3050432205200195\n",
      "INFO:__main__:Epoch 1, Batch 430, Loss: 1.5529935359954834\n",
      "INFO:__main__:Epoch 1, Batch 440, Loss: 1.5700750350952148\n",
      "INFO:__main__:Epoch 1, Batch 450, Loss: 1.3826569318771362\n",
      "INFO:__main__:Epoch 1, Batch 460, Loss: 1.2373220920562744\n",
      "INFO:__main__:Epoch 1, Batch 470, Loss: 1.3147468566894531\n",
      "INFO:__main__:Epoch 1, Batch 480, Loss: 1.3205243349075317\n",
      "INFO:__main__:Epoch 1, Batch 490, Loss: 1.4297407865524292\n",
      "INFO:__main__:Epoch 1, Batch 500, Loss: 1.5599061250686646\n",
      "INFO:__main__:Epoch 1, Batch 510, Loss: 1.6875540018081665\n",
      "INFO:__main__:Epoch 1, Batch 520, Loss: 1.4660499095916748\n",
      "INFO:__main__:Epoch 1, Batch 530, Loss: 1.4275909662246704\n",
      "INFO:__main__:Epoch 1, Batch 540, Loss: 1.4260317087173462\n",
      "INFO:__main__:Epoch 1, Batch 550, Loss: 1.482810139656067\n",
      "INFO:__main__:Epoch 1, Batch 560, Loss: 1.6093400716781616\n",
      "INFO:__main__:Epoch 1, Batch 570, Loss: 1.3581598997116089\n",
      "INFO:__main__:Epoch 1, Batch 580, Loss: 1.4911508560180664\n",
      "INFO:__main__:Epoch 1, Batch 590, Loss: 1.508484959602356\n",
      "INFO:__main__:Epoch 1, Batch 600, Loss: 1.3823081254959106\n",
      "INFO:__main__:Epoch 1, Batch 610, Loss: 1.427721381187439\n",
      "INFO:__main__:Epoch 1, Batch 620, Loss: 1.3141844272613525\n",
      "INFO:__main__:Epoch 1, Batch 630, Loss: 1.282335638999939\n",
      "INFO:__main__:Epoch 1, Batch 640, Loss: 1.428308367729187\n",
      "INFO:__main__:Epoch 1, Batch 650, Loss: 1.4892171621322632\n",
      "INFO:__main__:Epoch 1, Batch 660, Loss: 1.6310303211212158\n",
      "INFO:__main__:Epoch 1, Batch 670, Loss: 1.2790406942367554\n",
      "INFO:__main__:Epoch 1, Batch 680, Loss: 1.4244333505630493\n",
      "INFO:__main__:Epoch 1, Batch 690, Loss: 1.6048461198806763\n",
      "INFO:__main__:Epoch 1, Batch 700, Loss: 1.4934579133987427\n",
      "INFO:__main__:Epoch 1, Batch 710, Loss: 1.4781553745269775\n",
      "INFO:__main__:Epoch 1, Batch 720, Loss: 1.440354347229004\n",
      "INFO:__main__:Epoch 1, Batch 730, Loss: 1.5214128494262695\n",
      "INFO:__main__:Epoch 1, Batch 740, Loss: 1.4073505401611328\n",
      "INFO:__main__:Epoch 1, Batch 750, Loss: 1.3408541679382324\n",
      "INFO:__main__:Epoch 1, Batch 760, Loss: 1.2952980995178223\n",
      "INFO:__main__:Epoch 1, Batch 770, Loss: 1.3662022352218628\n",
      "INFO:__main__:Epoch 1, Batch 780, Loss: 1.4126794338226318\n",
      "INFO:__main__:Epoch 1, Batch 790, Loss: 1.3260351419448853\n",
      "INFO:__main__:Epoch 1, Batch 800, Loss: 1.3785665035247803\n",
      "INFO:__main__:Epoch 1, Batch 810, Loss: 1.4331634044647217\n",
      "INFO:__main__:Epoch 1, Batch 820, Loss: 1.625234603881836\n",
      "INFO:__main__:Epoch 1, Batch 830, Loss: 1.6793243885040283\n",
      "INFO:__main__:Epoch 1, Batch 840, Loss: 1.3941770792007446\n",
      "INFO:__main__:Epoch 1, Batch 850, Loss: 1.3461644649505615\n",
      "INFO:__main__:Epoch 1, Batch 860, Loss: 1.361342191696167\n",
      "INFO:__main__:Epoch 1, Batch 870, Loss: 1.3651078939437866\n",
      "INFO:__main__:Epoch 1, Batch 880, Loss: 1.3815199136734009\n",
      "INFO:__main__:Epoch 1, Batch 890, Loss: 1.6588362455368042\n",
      "INFO:__main__:Epoch 1, Batch 900, Loss: 1.45881986618042\n",
      "INFO:__main__:Epoch 1, Batch 910, Loss: 1.4665606021881104\n",
      "INFO:__main__:Epoch 1, Batch 920, Loss: 1.4186601638793945\n",
      "INFO:__main__:Epoch 1, Batch 930, Loss: 1.437817931175232\n",
      "INFO:__main__:Epoch 1, Batch 940, Loss: 1.659868597984314\n",
      "INFO:__main__:Epoch 1, Batch 950, Loss: 1.681849718093872\n",
      "INFO:__main__:Epoch 1, Batch 960, Loss: 1.2567468881607056\n",
      "INFO:__main__:Epoch 1, Batch 970, Loss: 1.6084508895874023\n",
      "INFO:__main__:Epoch 1, Batch 980, Loss: 1.3881816864013672\n",
      "INFO:__main__:Epoch 1, Batch 990, Loss: 1.4780763387680054\n",
      "INFO:__main__:Epoch 1, Batch 1000, Loss: 1.652392864227295\n",
      "INFO:__main__:Epoch 1, Batch 1010, Loss: 1.3380937576293945\n",
      "INFO:__main__:Epoch 1, Batch 1020, Loss: 1.4882594347000122\n",
      "INFO:__main__:Epoch 1, Batch 1030, Loss: 1.5494855642318726\n",
      "INFO:__main__:Epoch 1, Batch 1040, Loss: 1.4300944805145264\n",
      "INFO:__main__:Epoch 1, Batch 1050, Loss: 1.5079905986785889\n",
      "INFO:__main__:Epoch 1, Batch 1060, Loss: 1.7364403009414673\n",
      "INFO:__main__:Epoch 1, Batch 1070, Loss: 1.6494401693344116\n",
      "INFO:__main__:Epoch 1, Batch 1080, Loss: 1.411119818687439\n",
      "INFO:__main__:Epoch 1, Batch 1090, Loss: 1.5247457027435303\n",
      "INFO:__main__:Epoch 1, Batch 1100, Loss: 1.3051811456680298\n",
      "INFO:__main__:Epoch 1, Batch 1110, Loss: 1.4442211389541626\n",
      "INFO:__main__:Epoch 1, Batch 1120, Loss: 1.451892375946045\n",
      "INFO:__main__:Epoch 1, Batch 1130, Loss: 1.4264731407165527\n",
      "INFO:__main__:Epoch 1, Batch 1140, Loss: 1.4196256399154663\n",
      "INFO:__main__:Epoch 1, Batch 1150, Loss: 1.291687250137329\n",
      "INFO:__main__:Epoch 1, Batch 1160, Loss: 1.408554196357727\n",
      "INFO:__main__:Epoch 1, Batch 1170, Loss: 1.4107357263565063\n",
      "INFO:__main__:Epoch 1, Batch 1180, Loss: 1.2484272718429565\n",
      "INFO:__main__:Epoch 1, Batch 1190, Loss: 1.3652756214141846\n",
      "INFO:__main__:Epoch 1, Batch 1200, Loss: 1.4010473489761353\n",
      "INFO:__main__:Epoch 1, Batch 1210, Loss: 1.3797919750213623\n",
      "INFO:__main__:Epoch 1, Batch 1220, Loss: 1.5894476175308228\n",
      "INFO:__main__:Epoch 1, Batch 1230, Loss: 1.4365125894546509\n",
      "INFO:__main__:Epoch 1, Batch 1240, Loss: 1.4621862173080444\n",
      "INFO:__main__:Epoch 1, Batch 1250, Loss: 1.3029658794403076\n",
      "INFO:__main__:Epoch 1, Batch 1260, Loss: 1.473541259765625\n",
      "INFO:__main__:Epoch 1, Batch 1270, Loss: 1.3356823921203613\n",
      "INFO:__main__:Epoch 1, Batch 1280, Loss: 1.4803105592727661\n",
      "INFO:__main__:Epoch 1, Loss: 1.4395635446653685, Accuracy: 0.3607405787593848\n",
      "INFO:__main__:Saved checkpoint to 'model_checkpoint_epoch_1.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and criterion\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize logging and TensorBoard writer\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "checkpoint_interval = 1  # For example, save the model every epoch\n",
    "\n",
    "# Training loop with accuracy calculation and checkpointing\n",
    "num_epochs = 2\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_epochs):  # Loop over the dataset multiple times, defined by num_epochs.\n",
    "    model.train()  # Set the model to training mode. This enables dropout and batch normalization layers.\n",
    "    total_loss = 0  # Initialize total loss for the epoch.\n",
    "    correct_predictions = 0  # Initialize the count of correct predictions for accuracy calculation.\n",
    "    total_predictions = 0  # Initialize the total number of predictions made.\n",
    "    \n",
    "    for batch_idx, (input_ids, labels) in enumerate(data_loader):  # Iterate over batches of data.\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)  # Move input and labels to the configured device (GPU/CPU).\n",
    "\n",
    "        optimizer.zero_grad()  # Clear the gradients before computing them. Prevents accumulation from previous iterations.\n",
    "        outputs = model(input_ids, attention_mask=None)  # Forward pass through the model. Attention mask is optional here.\n",
    "        \n",
    "        # Extract logits from model outputs. Assumes outputs are structured with logits as the first element.\n",
    "        logits = outputs[0][:, -1, :]  # Get logits for the last token positions across all examples in the batch.\n",
    "\n",
    "        loss = criterion(logits, labels)  # Compute the loss between model predictions and true labels.\n",
    "        loss.backward()  # Backpropagate the error through the model.\n",
    "        optimizer.step()  # Update model parameters based on gradients.\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss.\n",
    "\n",
    "        _, predicted_labels = torch.max(logits, 1)  # Get the predicted labels by finding the max logit value across columns.\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()  # Count correct predictions.\n",
    "        total_predictions += labels.size(0)  # Update the total number of predictions made.\n",
    "\n",
    "        if batch_idx % 10 == 0:  # Optionally log information every 10 batches.\n",
    "            logger.info(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')  # Log current epoch, batch, and loss.\n",
    "\n",
    "    epoch_loss = total_loss / len(data_loader)  # Calculate average loss for the epoch.\n",
    "    epoch_accuracy = correct_predictions / total_predictions  # Calculate accuracy for the epoch.\n",
    "\n",
    "    logger.info(f'Epoch {epoch}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}')  # Log the epoch's average loss and accuracy.\n",
    "    writer.add_scalar('Loss', epoch_loss, epoch)  # Write the epoch loss to TensorBoard.\n",
    "    writer.add_scalar('Accuracy', epoch_accuracy, epoch)  # Write the epoch accuracy to TensorBoard.\n",
    "\n",
    "    # Checkpointing: save the model state_dict every `checkpoint_interval` epochs.\n",
    "    if epoch % checkpoint_interval == 0:\n",
    "        checkpoint_path = f'model_checkpoint_epoch_{epoch}.pth'  # Define the checkpoint file path.\n",
    "        torch.save(model.state_dict(), checkpoint_path)  # Save the model's state_dict.\n",
    "        logger.info(f\"Saved checkpoint to '{checkpoint_path}'.\")  # Log that the model was successfully saved.\n",
    "\n",
    "    global_step += len(data_loader)  # Update the global step by the number of batches processed.\n",
    "\n",
    "writer.close()  # Close the TensorBoard writer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQGFhItvO6FT"
   },
   "source": [
    "Step 4. Monitoring and Logging:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-AptF_FWSYuu"
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # Inside the training loop\n",
    "# logger.info(f'Epoch {epoch}, Batch {batch}, Loss: {loss.item()}, Accuracy: {accuracy.item()}')\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Create a TensorBoard writer\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "# # Inside the training loop\n",
    "# writer.add_scalar('Loss', loss.item(), global_step)\n",
    "# writer.add_scalar('Accuracy', accuracy.item(), global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYdlWtNgUWTp"
   },
   "source": [
    "Step 5: Utilizing GPU/TPU resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "m4FlGe5yUcq5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#Using Tesorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Explicitly specify GPU device\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaTu5uH9mbL7"
   },
   "source": [
    "Step 6 : Regular Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eJJI2tg4mjdZ"
   },
   "outputs": [],
   "source": [
    "# sample_texts = ['This product has been great in my experience!', 'I really did not like this movie at all.']\n",
    "# predictions = [predict(text) for text in sample_texts]\n",
    "# for text, pred in zip(sample_texts, predictions):\n",
    "#     logger.info(f'Text: {text}, Predicted Sentiment: {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqxFIt9em59h"
   },
   "source": [
    "Step 7 : Ethical and Quality control in Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QAGOHN9km4Cw"
   },
   "outputs": [],
   "source": [
    "#Sample code for bias monitoring and data Quality control\n",
    "#Implement your own logic and bias monitoring for the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El-U0mEvncf6"
   },
   "source": [
    "Step 8: Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pOttH6ipne2H"
   },
   "outputs": [],
   "source": [
    "#Sample code for Code Documentation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
